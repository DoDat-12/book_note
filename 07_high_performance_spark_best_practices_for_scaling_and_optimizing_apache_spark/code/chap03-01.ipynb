{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403ed38a",
   "metadata": {},
   "source": [
    "# Chapter 3: DataFrames, Datasets, and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951a9594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.0`\n",
    "import $ivy.`org.apache.spark::spark-hive:3.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6b318e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20261555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{Dataset, DataFrame, SparkSession, Row}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.catalyst.expressions.aggregate._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Dataset, DataFrame, SparkSession, Row}\n",
    "import org.apache.spark.sql.catalyst.expressions.aggregate._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa38bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1a56a3ef"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"chap03-01\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df67e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12027bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_corrupt_record: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.read.json(\"data/rawpanda.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb929f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd5718",
   "metadata": {},
   "source": [
    "**JSON Example**\n",
    "\n",
    "```\n",
    "{\n",
    "    \"name\":\"mission\",\n",
    "    \"pandas\":[\n",
    "        {\n",
    "            \"id\":1,\n",
    "            \"zip\":\"94110\",\n",
    "            \"pt\":\"giant\",\n",
    "            \"happy\":true,\n",
    "            \"attributes\":[0.4,0.5]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Sample Schema Information for nested structure**\n",
    "\n",
    "```\n",
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- pandas: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- id: long (nullable = false)\n",
    " |    |    |-- zip: string (nullable = true)\n",
    " |    |    |-- pt: string (nullable = true)\n",
    " |    |    |-- happy: boolean (nullable = false)\n",
    " |    |    |-- attributes: array (nullable = true)\n",
    " |    |    |    |-- element: double (containsNull = false)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb91255f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRawPanda\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mPandaPlace\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Case class, can used to create Dataset and print the schema\n",
    "case class RawPanda(\n",
    "    id: Long,\n",
    "    zip: String,\n",
    "    pt: String,\n",
    "    happy: Boolean, \n",
    "    attributes: Array[Double]\n",
    ")\n",
    "\n",
    "case class PandaPlace(\n",
    "    name: String,\n",
    "    pandas: Array[RawPanda]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e3f33",
   "metadata": {},
   "source": [
    "**Create a Dataset with the case class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42dad3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// def createAndPrintSchema() = {\n",
    "//     val damao = RawPanda(1, \"M1B 5K7\", \"giant\", true, Array(0.1, 0.1))\n",
    "//     val pandaPlace = PandaPlace(\"toronro\", Array(damao))\n",
    "//     val df = spark.createDataFrame(Seq(pandaPlace))\n",
    "//     df.printSchema()\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cf90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "// createAndPrintSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27fcba2",
   "metadata": {},
   "source": [
    "**Sample schema information for nested structure (.schema())â€”manually formatted**\n",
    "\n",
    "```\n",
    "org.apache.spark.sql.types.StructType = StructType(\n",
    "    StructField(name,StringType,true),\n",
    "    StructField(pandas,\n",
    "        ArrayType(\n",
    "            StructType(StructField(id,LongType,false),\n",
    "                       StructField(zip,StringType,true),\n",
    "                       StructField(pt,StringType,true),\n",
    "                       StructField(happy,BooleanType,false),\n",
    "                       StructField(attributes,ArrayType(DoubleType,false),true)\n",
    "                      ),\n",
    "            true),\n",
    "        true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0523d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- pandas: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = false)\n",
      " |    |    |-- zip: string (nullable = true)\n",
      " |    |    |-- pt: string (nullable = true)\n",
      " |    |    |-- happy: boolean (nullable = false)\n",
      " |    |    |-- attributes: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdamao\u001b[39m: \u001b[32mRawPanda\u001b[39m = \u001b[33mRawPanda\u001b[39m(\u001b[32m1L\u001b[39m, \u001b[32m\"M1B 5K7\"\u001b[39m, \u001b[32m\"giant\"\u001b[39m, true, \u001b[33mArray\u001b[39m(\u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m))\n",
       "\u001b[36mpandaPlace\u001b[39m: \u001b[32mPandaPlace\u001b[39m = \u001b[33mPandaPlace\u001b[39m(\n",
       "  \u001b[32m\"toronro\"\u001b[39m,\n",
       "  \u001b[33mArray\u001b[39m(\u001b[33mRawPanda\u001b[39m(\u001b[32m1L\u001b[39m, \u001b[32m\"M1B 5K7\"\u001b[39m, \u001b[32m\"giant\"\u001b[39m, true, \u001b[33mArray\u001b[39m(\u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m)))\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, pandas: array<struct<id:bigint,zip:string,pt:string,happy:boolean,attributes:array<double>>>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val damao = RawPanda(1, \"M1B 5K7\", \"giant\", true, Array(0.1, 0.1))\n",
    "val pandaPlace = PandaPlace(\"toronro\", Array(damao))\n",
    "val df = spark.createDataFrame(Seq(pandaPlace))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "886fc044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpandaInfo\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, pandas: array<struct<id:bigint,zip:string,pt:string,happy:boolean,attributes:array<double>>> ... 5 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select and explode operators\n",
    "val pandaInfo = df.explode(df(\"pandas\")){\n",
    "    case Row(pandas: Seq[Row]) =>\n",
    "        pandas.map{\n",
    "            case (Row(\n",
    "                id: Long, \n",
    "                zip: String,\n",
    "                pt: String,\n",
    "                happy: Boolean,\n",
    "                attrs: Seq[Double]\n",
    "            )) => RawPanda(id, zip, pt, happy, attrs.toArray)\n",
    "        }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ea73c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- pandas: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = false)\n",
      " |    |    |-- zip: string (nullable = true)\n",
      " |    |    |-- pt: string (nullable = true)\n",
      " |    |    |-- happy: boolean (nullable = false)\n",
      " |    |    |-- attributes: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = false)\n",
      " |-- id: long (nullable = false)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- pt: string (nullable = true)\n",
      " |-- happy: boolean (nullable = false)\n",
      " |-- attributes: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandaInfo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edf56da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [(attributes[0] / attributes[1]): double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandaInfo\n",
    "    .select((pandaInfo(\"attributes\")(0)/pandaInfo(\"attributes\")(1))).as(\"squishyness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc4779f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mencodePandaType\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// If/else in Spark SQL\n",
    "\n",
    "/**\n",
    "  * Encodes pandaType to Integer values instead of String values.\n",
    "  *\n",
    "  * @param pandaInfo the input DataFrame\n",
    "  * @return Returns a DataFrame of pandaId and integer value for pandaType.\n",
    "  */\n",
    "def encodePandaType(pandaInfo: DataFrame): DataFrame = {\n",
    "    pandaInfo.select(\n",
    "        pandaInfo(\"id\"), \n",
    "        (when(pandaInfo(\"pt\") === \"giant\", 0)\n",
    "            .when(pandaInfo(\"pt\") === \"red\", 1)\n",
    "            .otherwise(2)).as(\"encodedType\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03289e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
