{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78311f99",
   "metadata": {},
   "source": [
    "# Chapter 3: DataFrames, Datasets, and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c61e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.0`\n",
    "import $ivy.`org.apache.spark::spark-hive:3.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8611700c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f1a272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{Dataset, DataFrame, SparkSession, Row}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.catalyst.expressions.aggregate._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Dataset, DataFrame, SparkSession, Row}\n",
    "import org.apache.spark.sql.catalyst.expressions.aggregate._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618a3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@29f5863b"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"chap03-01\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce95f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53c0742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_corrupt_record: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.read.json(\"data/rawpanda.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba9e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978abbe",
   "metadata": {},
   "source": [
    "**JSON Example**\n",
    "\n",
    "```\n",
    "{\n",
    "    \"name\":\"mission\",\n",
    "    \"pandas\":[\n",
    "        {\n",
    "            \"id\":1,\n",
    "            \"zip\":\"94110\",\n",
    "            \"pt\":\"giant\",\n",
    "            \"happy\":true,\n",
    "            \"attributes\":[0.4,0.5]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Sample Schema Information for nested structure**\n",
    "\n",
    "```\n",
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- pandas: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- id: long (nullable = false)\n",
    " |    |    |-- zip: string (nullable = true)\n",
    " |    |    |-- pt: string (nullable = true)\n",
    " |    |    |-- happy: boolean (nullable = false)\n",
    " |    |    |-- attributes: array (nullable = true)\n",
    " |    |    |    |-- element: double (containsNull = false)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b3f706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRawPanda\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mPandaPlace\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Case class, can used to create Dataset and print the schema\n",
    "case class RawPanda(\n",
    "    id: Long,\n",
    "    zip: String,\n",
    "    pt: String,\n",
    "    happy: Boolean, \n",
    "    attributes: Array[Double]\n",
    ")\n",
    "\n",
    "case class PandaPlace(\n",
    "    name: String,\n",
    "    pandas: Array[RawPanda]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9bf64",
   "metadata": {},
   "source": [
    "**Create a Dataset with the case class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f342fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateAndPrintSchema\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createAndPrintSchema() = {\n",
    "    val damao = RawPanda(1, \"M1B 5K7\", \"giant\", true, Array(0.1, 0.1))\n",
    "    val pandaPlace = PandaPlace(\"toronro\", Array(damao))\n",
    "    val df = spark.createDataFrame(Seq(pandaPlace))\n",
    "    df.printSchema()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f4705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- pandas: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = false)\n",
      " |    |    |-- zip: string (nullable = true)\n",
      " |    |    |-- pt: string (nullable = true)\n",
      " |    |    |-- happy: boolean (nullable = false)\n",
      " |    |    |-- attributes: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createAndPrintSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543d8c2",
   "metadata": {},
   "source": [
    "**Sample schema information for nested structure (.schema())â€”manually formatted**\n",
    "\n",
    "```\n",
    "org.apache.spark.sql.types.StructType = StructType(\n",
    "    StructField(name,StringType,true),\n",
    "    StructField(pandas,\n",
    "        ArrayType(\n",
    "            StructType(StructField(id,LongType,false),\n",
    "                       StructField(zip,StringType,true),\n",
    "                       StructField(pt,StringType,true),\n",
    "                       StructField(happy,BooleanType,false),\n",
    "                       StructField(attributes,ArrayType(DoubleType,false),true)\n",
    "                      ),\n",
    "            true),\n",
    "        true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82cce6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cedb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
